{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drebin_dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnDuBAO_3Do2"
      },
      "source": [
        "Il faut installer keras_metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFFbC0FC10Nc",
        "outputId": "c6132770-4081-4fb3-bc8d-a01e3ddd9a2e"
      },
      "source": [
        "pip install keras_metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras_metrics) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Keras>=2.1.5->keras_metrics) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.1.5->keras_metrics) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->Keras>=2.1.5->keras_metrics) (1.5.2)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpIkiOeQ1TXC",
        "outputId": "75252aa0-55c5-4abb-fd3f-a6859ba0edee"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "from sklearn.metrics import precision_score as precision\n",
        "from sklearn.metrics import recall_score as recall\n",
        "from sklearn.metrics import f1_score as f1\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "#We recover the data from Drebin dataset\n",
        "feature_of_counts = \"feature_vectors_counts.csv\"\n",
        "\n",
        "dataset = pd.read_csv(feature_of_counts, index_col=1)\n",
        "dataset = dataset.reindex(np.random.permutation(dataset.index))\n",
        "\n",
        "X = dataset.iloc[:,1:9].values\n",
        "y = dataset.iloc[:, 9].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y.astype(int), test_size = 0.3, random_state = 0)\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "#We create the model and train him\n",
        "\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(y_pred)\n",
        "#We print the result\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix', cm)\n",
        "\n",
        "accuracy = acc(y_test, y_pred)\n",
        "print('accuracy', accuracy)\n",
        "\n",
        "precision_score = precision(y_test, y_pred, average='micro')\n",
        "print('precision', precision_score)\n",
        "\n",
        "recall_score = recall(y_test, y_pred)\n",
        "print('recall', recall_score)\n",
        "\n",
        "f1_score = f1(y_test, y_pred)\n",
        "print('f1', f1_score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 1 0 1]\n",
            "Confusion Matrix [[15826   894]\n",
            " [  397   322]]\n",
            "accuracy 0.9259705258329033\n",
            "precision 0.9259705258329033\n",
            "recall 0.4478442280945758\n",
            "f1 0.33281653746770024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShYfIqeu2YPr",
        "outputId": "fb5040c6-110f-4456-9398-e00bf2445a17"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "from sklearn.metrics import precision_score as precision\n",
        "from sklearn.metrics import recall_score as recall\n",
        "from sklearn.metrics import f1_score as f1\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#We recover the data\n",
        "feature_of_counts = \"feature_vectors_counts.csv\"\n",
        "\n",
        "dataset = pd.read_csv(feature_of_counts, index_col=0)\n",
        "X = dataset.iloc[:,1:9].values\n",
        "y = dataset.iloc[:, 9].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y.astype(int), test_size = 0.3)\n",
        "\n",
        "#We create the model and train him\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "classifier = SVC(kernel = 'poly', degree=3, verbose = 1)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(y_pred)\n",
        "\n",
        "#We print the result\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "accuracy = acc(y_test, y_pred)\n",
        "print('accuracy', accuracy)\n",
        "\n",
        "precision_score = precision(y_test, y_pred, average='micro')\n",
        "print('precision', precision_score)\n",
        "\n",
        "recall_score = recall(y_test, y_pred)\n",
        "print('recall', recall_score)\n",
        "\n",
        "f1_score = f1(y_test, y_pred)\n",
        "print('f1', f1_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM][0 0 0 ... 0 0 0]\n",
            "[[16607    25]\n",
            " [  677   130]]\n",
            "accuracy 0.9597453982453122\n",
            "precision 0.9597453982453122\n",
            "recall 0.161090458488228\n",
            "f1 0.2702702702702703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTADFB983i8X"
      },
      "source": [
        "Tout comme pour l'étude faites par Drebin, nous obtenons des résultats relativement similaires, nous constaterons toutefois que les recall sont mauvais et que ces méthodes génèrent énormément de faux négatifs, ce qui est problématique. Il doit y avoir une erreur dans mon traîtement de données ou dans ma façon d'entraîner mes modèles."
      ]
    }
  ]
}